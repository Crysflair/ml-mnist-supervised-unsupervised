{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN+MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crysflair/ml-mnist-supervised-unsupervised/blob/master/CNN%2BMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BeUEBV_FSmSM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN + MNIST for AI course work (2018, BIT)\n",
        "\n",
        "This is a practice of tensorflow and tensorboard.  \n",
        "\n",
        "Note there is still something wrong with the code: tensorboard fails to show many curves as I want. \n",
        "\n",
        "It might dues to namespace's issue?"
      ]
    },
    {
      "metadata": {
        "id": "PCIWk11vzSO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = \"./MO_MNIST_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QyEszWBZsWRY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data  # download and extract the data set automatically\n",
        "\n",
        "\n",
        "#### DEFINE FREQUENTLY USED FUNCTIONS ####\n",
        "\n",
        "# initialize every weight matrix as truncated normal for relu.\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "# initialize every bias vector as a constant.\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltHcu--H4Aku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "    with tf.name_scope('mo_auto_summaries'):\n",
        "      mean = tf.reduce_mean(var)\n",
        "      tf.summary.scalar('mean', mean)\n",
        "      with tf.name_scope('stddev'):\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        \n",
        "      # 使用tf.summary.scaler记录记录下标准差，最大值，最小值\n",
        "      tf.summary.scalar('stddev', stddev)\n",
        "      tf.summary.scalar('max', tf.reduce_max(var))\n",
        "      tf.summary.scalar('min', tf.reduce_min(var))\n",
        "      \n",
        "      # 用直方图记录参数的分布\n",
        "      tf.summary.histogram('histogram', var)\n",
        "      \n",
        "# reference: https://blog.csdn.net/sinat_33761963/article/details/62433234 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5gGMyePwtI5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### CONSTRUCT THE MODEL ####\n",
        "# reference: https://www.cnblogs.com/Ran-Chen/p/9220739.html\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# get the data source\n",
        "mnist = input_data.read_data_sets(data_dir, one_hot=True);\n",
        "\n",
        "# input image:pixel 28*28 = 784\n",
        "with tf.name_scope('input'):\n",
        "    x = tf.placeholder(tf.float32, [None, 784])\n",
        "    y_ = tf.placeholder('float', [None, 10])  # y_ is realistic result\n",
        "\n",
        "with tf.name_scope('image'):\n",
        "    x_image = tf.reshape(x, [-1, 28, 28, 1])  # any dim, width, height, channel(depth)\n",
        "    tf.summary.image('input_image', x_image, 8)\n",
        "\n",
        "\n",
        "# the first convolution layer\n",
        "with tf.name_scope('conv_layer1'):\n",
        "    W_conv1 = weight_variable([5, 5, 1, 32])  # convolution kernel: 5*5*1, number of kernel: 32\n",
        "    variable_summaries(W_conv1)\n",
        "    b_conv1 = bias_variable([32])\n",
        "    # variable_summaries(b_conv1)\n",
        "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  # make convolution, output: 28*28*32\n",
        "\n",
        "with tf.name_scope('pooling_layer'):\n",
        "    h_pool1 = max_pool_2x2(h_conv1)  # make pooling, output: 14*14*32\n",
        "\n",
        "# the second convolution layer\n",
        "with tf.name_scope('conv_layer2'):\n",
        "    W_conv2 = weight_variable([5, 5, 32, 64])  # convolution kernel: 5*5, depth: 32, number of kernel: 64\n",
        "    b_conv2 = bias_variable([64])\n",
        "    variable_summaries(W_conv2)\n",
        "    # variable_summaries(b_conv2)\n",
        "    \n",
        "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  # output: 14*14*64\n",
        "\n",
        "with tf.name_scope('pooling_layer'):\n",
        "    h_pool2 = max_pool_2x2(h_conv2)  # output: 7*7*64\n",
        "\n",
        "    \n",
        "# the first fully connected layer\n",
        "with tf.name_scope('fc_layer3'):\n",
        "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
        "    b_fc1 = bias_variable([1024])  # size: 1*1024\n",
        "    variable_summaries(W_fc1)\n",
        "    # variable_summaries(b_fc1)\n",
        "    \n",
        "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # output: 1*1024\n",
        "\n",
        "# dropout\n",
        "with tf.name_scope('dropout'):\n",
        "    keep_prob = tf.placeholder(tf.float32)\n",
        "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "# the second fully connected layer\n",
        "with tf.name_scope('output_fc_layer4'):\n",
        "    W_fc2 = weight_variable([1024, 10])\n",
        "    b_fc2 = bias_variable([10])  # size: 1*10\n",
        "    variable_summaries(W_fc2)\n",
        "    # variable_summaries(b_fc2)\n",
        "\n",
        "with tf.name_scope('softmax'):\n",
        "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  # output: 1*10\n",
        "\n",
        "with tf.name_scope('lost'):\n",
        "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
        "    tf.summary.scalar('lost', cross_entropy)\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "with tf.name_scope('accuracy'):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    tf.summary.scalar('accuracy', accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ojS6PkkftfHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### PREPAREMENT BEFORE TRAINING ####\n",
        "\n",
        "# merge everything\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "# init all variables\n",
        "init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsXjR99A-VhO",
        "colab_type": "code",
        "outputId": "d3e7c7ce-cfaf-4846-c048-6c04cf6d8761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorboardcolab\n",
        "tbc = tensorboardcolab.TensorBoardColab()\n",
        "train_summary = tbc.get_writer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://dbe4ac49.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fKtFxu8muEbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run session\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    train_summary.add_graph(sess.graph)\n",
        "    \n",
        "    sess.run(init)\n",
        "    for i in range(2000):  # train 2000 times\n",
        "        batch = mnist.train.next_batch(50)\n",
        "        result, _ = sess.run([merged, train_step], feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
        "       \n",
        "        if i % 20 == 0:\n",
        "            # print train accuracy\n",
        "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})  # no dropout\n",
        "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
        "\n",
        "            # summary\n",
        "            train_summary.add_summary(result, i)\n",
        "    \n",
        "    # close summary to free resources\n",
        "    train_summary.close() \n",
        "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}